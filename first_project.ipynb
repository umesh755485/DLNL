{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1xd67XhpyuvOCmgHJigF1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umesh755485/DLNL/blob/main/first_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LKLvH1GpLWiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a734ae-ab4c-48cb-935a-795946facb87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raja\n",
            "and\n",
            "rani\n",
            "are\n",
            "such\n",
            "a\n",
            "damn\n",
            "coder\n"
          ]
        }
      ],
      "source": [
        "#stemming\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "sentence = \"raja and rani are such a damn coders\"\n",
        "for word in sentence.split():\n",
        "  print(ps.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatization"
      ],
      "metadata": {
        "id": "pU6SiX7c2yWw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"coder\", pos=\"n\")) # Changed 'u' to 'n' for noun\n",
        "print(lemmatizer.lemmatize(\"brother\",pos=\"n\")) # Changed 'm' to 'n' for noun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dzdJeEW22pD",
        "outputId": "7f1ff2b3-b6bb-4f4e-b148-ae79a7e502c2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coder\n",
            "brother\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removal of stop words\n"
      ],
      "metadata": {
        "id": "IUuL3-rT250v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('tamil'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRZdAw604dTU",
        "outputId": "16b9bcac-4fbf-4f33-a732-a56427db61e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['அங்கு', 'அங்கே', 'அடுத்த', 'அதனால்', 'அதன்', 'அதற்கு', 'அதிக', 'அதில்', 'அது', 'அதே', 'அதை', 'அந்த', 'அந்தக்', 'அந்தப்', 'அன்று', 'அல்லது', 'அவன்', 'அவரது', 'அவர்', 'அவர்கள்', 'அவள்', 'அவை', 'ஆகிய', 'ஆகியோர்', 'ஆகும்', 'இங்கு', 'இங்கே', 'இடத்தில்', 'இடம்', 'இதனால்', 'இதனை', 'இதன்', 'இதற்கு', 'இதில்', 'இது', 'இதை', 'இந்த', 'இந்தக்', 'இந்தத்', 'இந்தப்', 'இன்னும்', 'இப்போது', 'இரு', 'இருக்கும்', 'இருந்த', 'இருந்தது', 'இருந்து', 'இவர்', 'இவை', 'உன்', 'உள்ள', 'உள்ளது', 'உள்ளன', 'எந்த', 'என', 'எனக்', 'எனக்கு', 'எனப்படும்', 'எனவும்', 'எனவே', 'எனினும்', 'எனும்', 'என்', 'என்ன', 'என்னும்', 'என்பது', 'என்பதை', 'என்ற', 'என்று', 'என்றும்', 'எல்லாம்', 'ஏன்', 'ஒரு', 'ஒரே', 'ஓர்', 'கொண்ட', 'கொண்டு', 'கொள்ள', 'சற்று', 'சிறு', 'சில', 'சேர்ந்த', 'தனது', 'தன்', 'தவிர', 'தான்', 'நான்', 'நாம்', 'நீ', 'பற்றி', 'பற்றிய', 'பல', 'பலரும்', 'பல்வேறு', 'பின்', 'பின்னர்', 'பிற', 'பிறகு', 'பெரும்', 'பேர்', 'போது', 'போன்ற', 'போல', 'போல்', 'மட்டுமே', 'மட்டும்', 'மற்ற', 'மற்றும்', 'மிக', 'மிகவும்', 'மீது', 'முதல்', 'முறை', 'மேலும்', 'மேல்', 'யார்', 'வந்த', 'வந்து', 'வரும்', 'வரை', 'வரையில்', 'விட', 'விட்டு', 'வேண்டும்', 'வேறு']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#practical 2\n"
      ],
      "metadata": {
        "id": "RqZidhrX5x6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "print(\"vocabulary:\",vectorizer.vocabulary_)\n",
        "vector = vectorizer.transform(corpus)\n",
        "print(\"encode corpus is:\")\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRopK9ks52M-",
        "outputId": "228f1e81-7d44-4574-b818-553a661fbae3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary: {'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
            "encode corpus is:\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ]
    }
  ]
}