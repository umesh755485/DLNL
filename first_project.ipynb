{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJJIHegld1le7T1yLobQUY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umesh755485/DLNL/blob/main/first_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LKLvH1GpLWiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3b7055-d8c5-4db4-a14e-0400dc9d4a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raja\n",
            "and\n",
            "rani\n",
            "are\n",
            "such\n",
            "a\n",
            "damn\n",
            "coder\n"
          ]
        }
      ],
      "source": [
        "#stemming\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "sentence = \"raja and rani are such a damn coders\"\n",
        "for word in sentence.split():\n",
        "  print(ps.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatization"
      ],
      "metadata": {
        "id": "pU6SiX7c2yWw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"coder\", pos=\"n\")) # Changed 'u' to 'n' for noun\n",
        "print(lemmatizer.lemmatize(\"brother\",pos=\"n\")) # Changed 'm' to 'n' for noun"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dzdJeEW22pD",
        "outputId": "48cbf7f1-7866-4c34-ca08-04c2768ed584"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "coder\n",
            "brother\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#removal of stop words\n"
      ],
      "metadata": {
        "id": "IUuL3-rT250v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('tamil'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRZdAw604dTU",
        "outputId": "b01ac433-b318-449a-c22f-287b6dc87710"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['அங்கு', 'அங்கே', 'அடுத்த', 'அதனால்', 'அதன்', 'அதற்கு', 'அதிக', 'அதில்', 'அது', 'அதே', 'அதை', 'அந்த', 'அந்தக்', 'அந்தப்', 'அன்று', 'அல்லது', 'அவன்', 'அவரது', 'அவர்', 'அவர்கள்', 'அவள்', 'அவை', 'ஆகிய', 'ஆகியோர்', 'ஆகும்', 'இங்கு', 'இங்கே', 'இடத்தில்', 'இடம்', 'இதனால்', 'இதனை', 'இதன்', 'இதற்கு', 'இதில்', 'இது', 'இதை', 'இந்த', 'இந்தக்', 'இந்தத்', 'இந்தப்', 'இன்னும்', 'இப்போது', 'இரு', 'இருக்கும்', 'இருந்த', 'இருந்தது', 'இருந்து', 'இவர்', 'இவை', 'உன்', 'உள்ள', 'உள்ளது', 'உள்ளன', 'எந்த', 'என', 'எனக்', 'எனக்கு', 'எனப்படும்', 'எனவும்', 'எனவே', 'எனினும்', 'எனும்', 'என்', 'என்ன', 'என்னும்', 'என்பது', 'என்பதை', 'என்ற', 'என்று', 'என்றும்', 'எல்லாம்', 'ஏன்', 'ஒரு', 'ஒரே', 'ஓர்', 'கொண்ட', 'கொண்டு', 'கொள்ள', 'சற்று', 'சிறு', 'சில', 'சேர்ந்த', 'தனது', 'தன்', 'தவிர', 'தான்', 'நான்', 'நாம்', 'நீ', 'பற்றி', 'பற்றிய', 'பல', 'பலரும்', 'பல்வேறு', 'பின்', 'பின்னர்', 'பிற', 'பிறகு', 'பெரும்', 'பேர்', 'போது', 'போன்ற', 'போல', 'போல்', 'மட்டுமே', 'மட்டும்', 'மற்ற', 'மற்றும்', 'மிக', 'மிகவும்', 'மீது', 'முதல்', 'முறை', 'மேலும்', 'மேல்', 'யார்', 'வந்த', 'வந்து', 'வரும்', 'வரை', 'வரையில்', 'விட', 'விட்டு', 'வேண்டும்', 'வேறு']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#practical 2\n"
      ],
      "metadata": {
        "id": "RqZidhrX5x6g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "print(\"vocabulary:\",vectorizer.vocabulary_)\n",
        "vector = vectorizer.transform(corpus)\n",
        "print(\"encode corpus is:\")\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRopK9ks52M-",
        "outputId": "872d023a-7097-4199-96ac-2ada8dc2a471"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary: {'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
            "encode corpus is:\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>pratical 3</h1>\n"
      ],
      "metadata": {
        "id": "QeBgRqB2CuUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    \"sometimes life can get confusing and hard\",\n",
        "    \"in such times it can be useful to turn to the wisdom of poetry\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "mykeys = list(vectorizer.vocabulary_.keys())\n",
        "mykeys.sort()\n",
        "sorted_dict = {i: vectorizer.vocabulary_[i] for i in mykeys}\n",
        "print(\"vocabulary-\",sorted_dict)\n",
        "print(vectorizer.idf_)\n",
        "vector = vectorizer.transform([corpus[0]])\n",
        "print()\n",
        "print(vector.shape)\n",
        "print()\n",
        "print(\"encoded corpus is:-\")\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XefQLMfYC7pq",
        "outputId": "1f9ee302-c5f9-4059-c160-84451fdc4259"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary- {'and': 0, 'be': 1, 'can': 2, 'confusing': 3, 'get': 4, 'hard': 5, 'in': 6, 'it': 7, 'life': 8, 'of': 9, 'poetry': 10, 'sometimes': 11, 'such': 12, 'the': 13, 'times': 14, 'to': 15, 'turn': 16, 'useful': 17, 'wisdom': 18}\n",
            "[1.40546511 1.40546511 1.         1.40546511 1.40546511 1.40546511\n",
            " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
            " 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511 1.40546511\n",
            " 1.40546511]\n",
            "\n",
            "(1, 19)\n",
            "\n",
            "encoded corpus is:-\n",
            "[[0.39204401 0.         0.27894255 0.39204401 0.39204401 0.39204401\n",
            "  0.         0.         0.39204401 0.         0.         0.39204401\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> pratical 5 </h1>"
      ],
      "metadata": {
        "id": "yz80130-LO2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "text = \"Sometimes life can get confusing and hard\"\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "8bFkRIQzLYRO",
        "outputId": "07804114-43d6-478c-e34a-c036ccaa93be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    }
  ]
}